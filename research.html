<!DOCTYPE html><html data-bs-theme="light" lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no"><title>Projects - CamRAL</title><link rel="canonical" href="https://camral.github.io/research.html"><meta property="og:url" content="https://camral.github.io/research.html"><meta name="twitter:title" content="CamRAL Lab"><meta name="twitter:description" content="Cambridge Resilient Autonomous Learning Lab"><meta name="twitter:card" content="summary"><meta property="og:type" content="website"><meta name="twitter:image" content="https://camral.github.io/assets/img/index/camral_logo_button_only_robot.png"><meta property="og:title" content="CamRAL Lab"><meta name="description" content="Cambridge Resilient Autonomous Learning Lab"><meta property="og:image" content="https://camral.github.io/assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png" media="(prefers-color-scheme: dark)"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png" media="(prefers-color-scheme: dark)"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css"><link rel="manifest" href="manifest.json" crossorigin="use-credentials"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800&amp;display=swap"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bellefair&amp;display=swap"><link rel="stylesheet" href="assets/css/styles.min.css"></head><body><nav class="navbar navbar-expand-md sticky-top fs-5 fw-semibold py-1 navbar-shrink" id="mainNav"><div class="container"><a class="navbar-brand d-flex align-items-center" href="/"><h1 class="text-uppercase" style="font-family:Bellefair, serif;color:var(--bs-primary);font-size:35pt;background-color:rgba(255, 255, 255, 0.8);padding:0px;border-radius:5px;"><span class="text-start" style="color: var(--bs-secondary);font-size: 25pt;">C</span><span class="text-start" style="color: var(--bs-secondary);font-size: 22pt;">AM</span><span class="text-start" style="color: var(--bs-primary);font-size: 25pt;">RAL</span></h1><img class="d-inline-block align-top" src="assets/img/index/camral_logo_button_only_robot.png" width="58" height="59" alt="CamRAL Group Logo: a robot with a red backpack reading a red book"></a><button data-bs-toggle="collapse" class="navbar-toggler" data-bs-target="#navcol-1"><span class="visually-hidden">Toggle navigation</span><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse ms-0 me-0" id="navcol-1" title=""><ul class="navbar-nav fw-semibold ms-auto"><li class="nav-item"><a class="nav-link" href="index.html" style="color: var(--bs-secondary);" data-bs-toggle="/index.html/">Home</a></li><li class="nav-item" style="color: var(--bs-secondary);"><a class="nav-link" href="index.html#team" style="color: var(--bs-secondary);">Team</a></li><li class="nav-item" style="color: var(--bs-secondary);"><a class="nav-link active" href="research.html" style="color: var(--bs-secondary);">Research</a></li><li class="nav-item" style="color: var(--bs-secondary);"><a class="nav-link" href="collaborations.html" style="color: var(--bs-secondary);">Collaborations</a></li><li class="nav-item" style="color: var(--bs-secondary);"><a class="nav-link" href="teaching.html" style="color: var(--bs-secondary);">Teaching</a></li></ul><a href="/research.html"></a><a href="#"></a></div></div></nav><div class="container py-5" id="team"><div class="row mb-2"><div class="col-md-8 col-xl-6 text-center mx-auto"><h2 class="mt-0 pt-0 mx-0"><strong>Research Themes</strong></h2><p></p></div></div><div class="row gy-4 row-cols-1 row-cols-sm-1 row-cols-md-1 row-cols-lg-1 row-cols-xl-1 row-cols-xxl-1"><div class="col"><div class="d-flex flex-column flex-md-row align-items-md-center"><img class="img-fluid shadow flex-shrink-0 me-md-3 mb-3 mb-md-0" src="assets/img/research/research_ada_go.png" style="width: 400px;height: 200px;object-fit: cover;border-radius: 16px;box-shadow: inset 109px 0px 0px;" alt="A descriptive alt text"><div><h5 class="fs-4 fw-bold">Ada-GO&nbsp;&nbsp;</h5><p class="mb-4">We are creating a global optimization framework for co-evolving robot hardware (sensors, morphology, actuation, materials) and the reinforcement learning methods that control it. The goal is to make this co-evolution efficient through adaptive simulation, active learning, and hardware-in-the-loop optimization. By performing this search at a global scale, we aim to uncover novel, low-cost robot designs and powerful RL techniques that significantly outperform existing solutions. Ultimately, the aim is to drastically reduce the cost and time required to customize robots.</p><p class="fs-6 fw-normal"><a class="fs-6 fw-normal link-secondary" href="https://www.aria.org.uk/opportunity-spaces/smarter-robot-bodies/robot-dexterity/meet-the-creators?cardId=ada-go"><span style="font-weight: normal !important;">ARIA</span></a>&nbsp;/&nbsp;<a class="fs-6 fw-normal link-secondary" href="https://ee.eng.cam.ac.uk/index.php/2025/05/29/dexterity-by-design-three-cambridge-robotics-projects-part-of-57m-aria-programme/">Electrical Engineering News</a>&nbsp;/&nbsp;<a class="fs-6 fw-normal link-secondary" href="https://www.cst.cam.ac.uk/news/transforming-robot-dexterity">Computer Science News</a>&nbsp;/&nbsp;<a class="fs-6 fw-normal link-secondary" href="https://observer.co.uk/news/business/article/robots-to-be-given-tactile-skin-and-dexterous-hands">The Observer</a></p></div></div></div><div class="col"><div class="d-flex flex-column flex-md-row align-items-md-center"><img class="img-fluid shadow flex-shrink-0 me-md-3 mb-3 mb-md-0" src="assets/img/research/research_local_int.png" style="width: 400px;height: 200px;object-fit: cover;border-radius: 16px;" alt="A descriptive alt text"><div><h5 class="fs-4 fw-bold">Local Intelligence</h5><p class="mb-4">Today’s autonomous systems are dependent on a stable connection to cloud servers, making them brittle, insecure, and inaccessible where connectivity is limited. Encode Fellow Martyna Stachaczyk is working with our team to compress large vision-language models for edge deployment and to design a fully on-device control architecture for real-time local intelligence. This research could free intelligent systems from the cloud, enabling safe, private, and adaptive autonomy for edge devices, even in resource-constrained or offline settings.</p><p class="fs-6 fw-normal"><a class="fs-6 fw-normal link-secondary" href="https://www.cam.ac.uk/news/new-encode-fellowships-boost-ai-research-at-cambridge"><span style="font-weight: normal !important;">ARIA</span></a>&nbsp;/&nbsp;<a class="fs-6 fw-normal link-secondary" href="https://encode.pillar.vc/projects/autonomous-edge-intelligence">Encode</a>&nbsp;/&nbsp;<a class="fs-6 fw-normal link-secondary" href="https://www.linkedin.com/posts/encode-ai-for-science_on-tuesday-pillar-vc-was-invited-to-no10-activity-7351624153931644930-KXM2">LinkedIn Post</a></p></div></div></div><div class="col"><div class="d-flex flex-column flex-md-row align-items-md-center"><img class="img-fluid shadow flex-shrink-0 me-md-3 mb-3 mb-md-0" src="assets/img/research/research_aixsim.jpg" style="width: 400px;height: 200px;object-fit: cover;border-radius: 16px;" alt="A descriptive alt text"><div><h5 class="fs-4 fw-bold">AIxSIM</h5><p class="mb-4">With the emergence of various AI model scaling laws, a critical question arises: Can existing hardware sustain the continued growth of these models, or will a breakthrough architecture be required to deliver the same capabilities at radically lower costs? To explore such potentially transformative hardware designs, AIxSim—a project by teams from Imperial College London,&nbsp;the University of Edinburgh, and the University of Cambridge—aims to estimate their impact at both the system level (power and performance) and the model level (accuracy).</p><p class="fs-6 fw-normal"><a class="fs-6 fw-normal link-secondary" href="https://www.aria.org.uk/opportunity-spaces/nature-computes-better/scaling-compute/meet-the-creators?cardId=breaking-down-compute-graph"><span style="font-weight: normal !important;">ARIA</span></a>&nbsp;/&nbsp;<a class="fs-6 fw-normal link-secondary" href="https://aicrosssim.github.io/">Project Website</a></p></div></div></div></div></div><section style="--bs-body-bg: var(--bs-light);"><div class="container bg-transparent py-5 bg-primary-gradient" style="background: var(--bs-light);--bs-body-bg: var(--bs-light);"><div class="row"><div class="col-md-8 col-xl-6 text-center mx-auto"><h3 class="fw-bold">PhD Projects</h3></div></div><div class="p-lg-5 py-5"><div class="row row-cols-1 row-cols-md-2 mx-auto"><div class="col mb-5"><div class="card shadow-sm"><div class="card-body pt-2 px-4 px-md-5 py-0"><h5 class="fw-normal mt-3 card-title">Multimodal Sensing and Reasoning for Mobile Robot Manipulation</h5><p class="text-muted mb-4 card-text">PhD student: Hantao Zhong</p></div></div></div><div class="col mb-5"><div class="card shadow-sm"><div class="card-body h-100 px-4 px-md-5"><h5 class="fw-normal mt-3 card-title">Co-Design for Foundation Models and AI Accelerators</h5><p class="text-muted mb-4 card-text">PhD student: Jiayi Nie</p></div></div></div><div class="col mb-5"><div class="card shadow-sm"><div class="card-body px-4 px-md-5 py-3"><h5 class="fw-normal mt-3 card-title">Resource-constrained Continual Reinforcement Learning</h5><p class="text-muted mb-4 card-text">PhD student: Max Tamborski</p></div></div></div><div class="col mb-5"><div class="card shadow-sm"><div class="card-body px-4 px-md-5 py-3"><h5 class="fw-normal mt-3 card-title">Physics-informed Representations for Scalable Simulation</h5><p class="text-muted mb-4 card-text">PhD student: Avi Newatia</p></div></div></div><div class="col mb-5"><div class="card shadow-sm"><div class="card-body pt-2 px-4 px-md-5 py-0"><h5 class="fw-normal mt-3 card-title">Sensing, Learning, and Optimization for Dexterous Manipulation</h5><p class="text-muted mb-4 card-text">PhD student: Andy Zhou</p></div></div></div><div class="col mb-5"><div class="card shadow-sm"><div class="card-body px-4 px-md-5 py-3"><h5 class="fw-normal mt-3 card-title">Modular Hardware Design for Robot Dexterity</h5><p class="text-muted mb-4 card-text">PhD student: Austin Yang</p></div></div></div></div></div></div></section><footer class="pb-4 bg-primary-gradient" style="background: #ffffff;" data-bs-theme="light"><footer class="text-center bg-body"><div class="col"><ul class="list-inline my-2"></ul></div><div class="container"><div class="row row-cols-1 row-cols-lg-3"><div class="col"><p class="text-body my-2"></p></div><div class="col"><img class="object-fit-contain" src="assets/img/index/cambridge_logo.png" alt="Slide Image" width="211" height="59"></div><div class="col"></div></div></div></footer></footer><script src="assets/bootstrap/js/bootstrap.min.js"></script><script src="assets/js/script.min.js"></script></body></html>
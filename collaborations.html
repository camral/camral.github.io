<!DOCTYPE html><html data-bs-theme="light" lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no"><title>Collaborations - CamRAL</title><link rel="canonical" href="https://camral.github.io/collaborations.html"><meta property="og:url" content="https://camral.github.io/collaborations.html"><meta property="og:title" content="CamRAL Lab"><meta name="twitter:card" content="summary"><meta name="twitter:description" content="Cambridge Resilient Autonomous Learning Lab"><meta property="og:type" content="website"><meta name="twitter:image" content="https://camral.github.io/assets/img/index/camral_logo_button_only_robot.png"><meta name="twitter:title" content="CamRAL"><meta name="description" content="Cambridge Resilient Autonomous Learning Lab"><meta property="og:image" content="https://camral.github.io/assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png" media="(prefers-color-scheme: dark)"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png" media="(prefers-color-scheme: dark)"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="icon" type="image/png" sizes="252x242" href="assets/img/index/camral_logo_button_only_robot.png"><link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css"><link rel="manifest" href="manifest.json" crossorigin="use-credentials"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800&amp;display=swap"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bellefair&amp;display=swap"><link rel="stylesheet" href="assets/fonts/ionicons.min.css"><link rel="stylesheet" href="assets/css/styles.min.css"></head><body><nav class="navbar navbar-expand-md sticky-top fs-5 fw-semibold py-1 navbar-shrink" id="mainNav"><div class="container"><a class="navbar-brand d-flex align-items-center" href="/"><h1 class="text-uppercase" style="font-family:Bellefair, serif;color:var(--bs-primary);font-size:35pt;background-color:rgba(255, 255, 255, 0.8);padding:0px;border-radius:5px;"><span class="text-start" style="color: var(--bs-secondary);font-size: 25pt;">C</span><span class="text-start" style="color: var(--bs-secondary);font-size: 22pt;">AM</span><span class="text-start" style="color: var(--bs-primary);font-size: 25pt;">RAL</span></h1><img class="d-inline-block align-top" src="assets/img/index/camral_logo_button_only_robot.png" width="58" height="59" alt="CamRAL Group Logo: a robot with a red backpack reading a red book"></a><button data-bs-toggle="collapse" class="navbar-toggler" data-bs-target="#navcol-1"><span class="visually-hidden">Toggle navigation</span><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse text-start ms-0 me-0" id="navcol-1" title=""><ul class="navbar-nav"><li class="nav-item"><a class="nav-link" href="index.html" style="color: var(--bs-secondary);" data-bs-toggle="/index.html/">Home</a></li><li class="nav-item" style="color: var(--bs-secondary);"><a class="nav-link" href="index.html#team" style="color: var(--bs-secondary);">Team</a></li><li class="nav-item" style="color: var(--bs-secondary);"><a class="nav-link" href="research.html" style="color: var(--bs-secondary);">Research</a></li><li class="nav-item" style="color: var(--bs-secondary);"><a class="nav-link active" href="collaborations.html" style="color: var(--bs-secondary);">Collaborations</a></li><li class="nav-item" style="color: var(--bs-secondary);"><a class="nav-link" href="teaching.html" style="color: var(--bs-secondary);">Teaching</a></li></ul><a href="/research.html"></a><a href="#"></a></div></div></nav><section class="py-5"><div class="container" id="team-1"><div class="row justify-content-center mx-auto"><div class="col-12 align-items-center align-content-center align-self-center mb-5"><h2 class="fw-bold text-center mb-3"><strong>Collaborations with Computer Architecture teams at Cambridge</strong></h2><div class="d-flex ratio-16x9 flex-column align-items-center align-content-center"><p style="max-width: 900px;">We are collaborating&nbsp;with&nbsp;<strong>CASCADE</strong>, the Computer Architecture and Semiconductor Design Centre at the University of Cambridge, which sits at the&nbsp;intersection of robotics and computer architecture.</p><img class="rounded img-fluid shadow w-100" src="assets/img/collaborators/collab_comp_arch_main_image.png" style="max-width: 900px;background: #ee582d;" width="876" height="657" alt="Robot"><p class="fs-6 text-muted" style="max-width: 900px;">Photo:&nbsp;https://www.cst.cam.ac.uk/news/amd-supports-cascade-centre-advance-phd-research-computer-architecture-and-semiconductor-design</p><div style="max-width: 900px;"><a class="btn btn-primary btn-sm text-start p-2 me-1 mt-1" role="button" href="https://www.cl.cam.ac.uk/research/cascade/people/">&nbsp;&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>&nbsp;CASCADE group&nbsp;</a></div></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/pip.png" alt="Teaser image for paper" style="background: #ee582d;"></div><div class="col-md-9"><h5 class="fw-bold">PIP: An Ensemble of Programming-Idiom Predictors</h5><p class="fw-normal mb-2">M Karl, A W Chadwick, M Erdos, <strong>J Nie, R Antonova</strong>, T Jones, R Mullins.<em>&nbsp;6th Championship Branch Prediction</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://ericrotenberg.wordpress.ncsu.edu/files/2025/06/cbp2025-final19-Mose.pdf">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;CBP 2025 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://www.youtube.com/watch?v=roYZwwQM2Ro" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-videocam-outline me-1 mt-1"></i>&nbsp;Talk Video&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://www.repository.cam.ac.uk/items/7d79733c-6777-423e-8378-cdbc9c1f57ab" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<i class="icon ion-code"></i>&nbsp;Code&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/memory_wall.png" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">Combating the Memory Walls: Optimization Pathways for Long-Context Agentic LLM Inference</h5><p class="fw-normal mb-2">H Wu, C Xiao, <strong>J Nie</strong>, X Guo, B Lou, J&nbsp; T H Wong, Z Mo, C Zhang, P Forys, W Luk, H Fan, J Cheng, T M Jones, <strong>R Antonova</strong>, R Mullins, A Zhao. Preprint</p><div><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2509.09505" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-document-text me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a></div></div></div></div><div class="container pb-3" id="team-3"><hr style="opacity: 1;background: var(--bs-primary);height: 3px;color: var(--bs-primary);"><div class="row justify-content-center mt-0 mb-0 pb-0 mx-auto"><div class="col-12 align-items-center align-content-center align-self-center"><h2 class="fw-bold text-center mb-3"><strong>Collaborations with Autodiscovery</strong></h2><div class="d-flex ratio-16x9 flex-column align-items-center align-content-center mb-0"><p style="max-width: 900px;">We are collaborating with <strong>Autodiscovery</strong> on applications in humanoid robotics, teleoperation and whole-body control. Autodiscovery is a startup that supplies customizable robotics solutions in the UK.</p><img class="rounded img-fluid shadow w-100" src="assets/img/collaborators/autonomy.png" style="max-width: 900px;background: #ee582d;" width="876" height="657" alt="Robot"><div class="pt-2" style="max-width: 900px;"><a class="btn btn-primary btn-sm text-start p-2 me-1 mt-1 mb-1" role="button" href="https://autodiscovery.co.uk/usecase/solutions.html">&nbsp;&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>&nbsp;Autodiscovery website&nbsp;</a></div><p style="max-width: 900px;">Check out a few demos of projects we collaborated on!</p></div></div></div><div class="row justify-content-center pt-0 mb-0 pb-0 mx-auto" style="max-width: 930px;"><div class="col order-md-2"><a href="https://www.linkedin.com/posts/autodiscovery-robots_autodiscovery-h1-remote-control-activity-7267941760792776704-U-zU"><div class="card"><img class="card-img w-100 d-block rounded shadow" src="assets/img/collaborators/autodiscovery-1.png"><div class="card-img-overlay text-center shadow d-flex flex-column justify-content-center align-items-center p-4"><h4><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-play text-white" style="font-size: 70px;">
  <path d="M10.804 8 5 4.633v6.734zm.792-.696a.802.802 0 0 1 0 1.392l-6.363 3.692C4.713 12.69 4 12.345 4 11.692V4.308c0-.653.713-.998 1.233-.696z"></path>
</svg></h4><p></p></div></div></a></div><div class="col order-md-2"><a href="https://www.linkedin.com/posts/autodiscovery-robots_autodiscovery-robots-physicalintelligence-activity-7302486572199792640-PObD"><div class="card"><img class="card-img w-100 d-block rounded shadow" src="assets/img/collaborators/autodiscovery-2.png"><div class="card-img-overlay text-center d-flex flex-column justify-content-center align-items-center p-4"><h4><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-play text-white" style="font-size: 70px;">
  <path d="M10.804 8 5 4.633v6.734zm.792-.696a.802.802 0 0 1 0 1.392l-6.363 3.692C4.713 12.69 4 12.345 4 11.692V4.308c0-.653.713-.998 1.233-.696z"></path>
</svg></h4><p></p></div></div></a></div></div></div><div class="container" id="team"><hr style="opacity: 1;background: var(--bs-primary);height: 3px;color: var(--bs-primary);"><div class="row justify-content-center mx-auto" style="max-width:900px;"><div class="col-12 mb-5"><h2 class="fw-bold text-center mb-3">Work with collaborators at Stanford University</h2><div class="ratio ratio-16x9"><iframe allowfullscreen="" frameborder="0" src="https://www.youtube.com/embed/kfh4gfpCreo" class="embed-responsive-item" width="" height=""></iframe></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/c-pik.jpg" alt="Teaser image for paper"></div><div class="col-md-9"><h4 class="fw-bold"><a class="fs-5 text-decoration-none text-dark" href="#">Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel</a></h4><p class="fw-normal mb-2">C Morlans, M Yi, C Chen, S Wu, <strong>R Antonova</strong>, T Gerstenberg, J Bohg. <em>International Conference on Machine Learning (ICML), 2025</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://icml.cc/virtual/2025/poster/45328">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;ICML 2025 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/pdf/2505.22861" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://stanford-iprl-lab.github.io/causal-pik/" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/mobipi.png" alt="Teaser image for paper"></div><div class="col-md-9"><h4 class="fw-bold"><a class="fs-5 text-decoration-none text-dark" href="#">Mobi-pi: Mobilizing Your Robot Learning Policy</a></h4><p class="fw-normal mb-2">J Yang, I Huang, B Vu, M Bajracharya, <strong>R Antonova</strong>, J Bohg. <em>Conference on Robot Learning (CoRL), 2025</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://openreview.net/forum?id=LnryWopsfJ">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;CoRL 2025 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2505.23692" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://mobipi.github.io/" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/cupid.png" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">CUPID: Curating Data your Robot Loves with Influence Functions</h5><p class="fw-normal mb-2">C Agia, R Sinha, J Yang, <strong>R Antonova</strong>, M Pavone, H Nishimura, M Itkina, J Bohg. <em>Conference on Robot Learning (CoRL), 2025</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://openreview.net/forum?id=TqevdDMqrK">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;CoRL 2025 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2506.19121" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://cupid-curation.github.io/" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/equibot.jpg" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">EquiBot: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning</h5><p class="fw-normal mb-2">J Yang, Z Cao, C Deng, <strong>R Antonova</strong>, S Song, J Bohg.&nbsp;<em>Conference on Robot Learning (CoRL), 2024</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://openreview.net/forum?id=ueBmGhLOXP">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;CoRL 2024 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2407.01479" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://equi-bot.github.io/" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/sentinel.png" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold"><span style="color: rgba(var(--bs-dark-rgb), var(--bs-text-opacity));">Unpacking Failure Modes of Generative Policies: Runtime Monitoring of Consistency and Progress</span></h5><p class="fw-normal mb-2">C Agia, R Sinha, J Yang, Z Cao, <strong>R Antonova</strong>, M Pavone, J Bohg. <em>Conference on Robot Learning (CoRL), 2024</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://openreview.net/forum?id=yqLFb0RnDW">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;CoRL 2024 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2410.04640" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://sites.google.com/stanford.edu/sentinel" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/equivact.png" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">EquivAct: SIM(3)-Equivariant Visuomotor Policies beyond Rigid Object Manipulation</h5><p class="fw-normal mb-2">J Yang, C Deng, J Wu, <strong>R Antonova</strong>, L Guibas, J Bohg. <em>IEEE International Conference on Robotics and Automation (ICRA), 2024</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://ieeexplore.ieee.org/document/10611491">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;ICRA 2024 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2310.16050" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://equivact.github.io/" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/learning-tools.png" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">Learning Tool Morphology for Contact-Rich Manipulation Tasks with Differentiable Simulation</h5><p class="fw-normal mb-2">M Li, <strong>R Antonova</strong>, D Sadigh, J Bohg.&nbsp;<em>IEEE International Conference on Robotics and Automation (ICRA), 2023</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://ieeexplore.ieee.org/document/10161453">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;ICRA 2023 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2211.02201" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://sites.google.com/stanford.edu/learning-tool-morphology" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/tidybot.gif" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">TidyBot: Personalized Robot Assistance with Large Language Models</h5><p class="fw-normal mb-2">J Wu, <strong>R Antonova</strong>, A Kan, M Lepert, A Zeng, S Song, J Bohg, S Rusinkiewicz, T Funkhouser.&nbsp;<em>IEEE International Conference on Intelligent Robots and Systems (IROS), 2023. Journal version published in Autonomous Robots, 2023. </em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://link.springer.com/article/10.1007/s10514-023-10139-z">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;Autonomous Robots Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2305.05658" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://tidybot.cs.princeton.edu/" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/inhand.png" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">In-Hand Manipulation of Unknown Objects with Tactile Sensing for Insertion</h5><p class="fw-normal mb-2">C Pan, M Lepert, S Yuan, <strong>R Antonova</strong>, J Bohg.&nbsp;<em>IEEE International Conference on Intelligent Robots and Systems (IROS), 2023</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://ieeexplore.ieee.org/document/10341456/">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;IROS 2023 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2210.13403" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://sites.google.com/stanford.edu/taskdriven-inhandmanip" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/rethinking.jpg" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">Rethinking Optimization with Differentiable Simulation from a Global Perspective</h5><p class="fw-normal mb-2"><strong>R Antonova*,</strong> J Yang*, K Jatavallabhula, J Bohg.&nbsp;<em>Conference on Robot Learning (CoRL), 2022</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://openreview.net/forum?id=Y_YUEEQMjQK">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;CoRL 2022 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2207.00167" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://yjy0625.github.io/projects/globdiff/" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/bayesian_treatment.png" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">A Bayesian Treatment of Real-to-Sim for Deformable Object Manipulation</h5><p class="fw-normal mb-2"><strong>R. Antonova</strong>, J. Yang, P. Sundaresan, D. Fox, F. Ramos, J. Bohg.&nbsp;<em>IEEE Robotics and Automation Letters (RA-L), 2022</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://ieeexplore.ieee.org/document/9730061">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;RA-L 2022 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2112.05068" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/diffcloud.jpg" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects</h5><p class="fw-normal mb-2">P Sundaresan, <strong>R Antonova</strong>, J Bohg.&nbsp;<em>IEEE International Conference on Intelligent Robots and Systems (IROS), 2022</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://ieeexplore.ieee.org/abstract/document/9981101">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;IROS 2022 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2204.03139" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://diffcloud.github.io/" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div><div class="row align-items-center mt-3 mb-5"><div class="col-md-3"><img class="rounded img-fluid" src="assets/img/collaborators/learningperiodictasks.png" alt="Teaser image for paper"></div><div class="col-md-9"><h5 class="fw-bold">Learning Periodic Tasks from Human Demonstrations</h5><p class="fw-normal mb-2">J Yang, J Zhang, C Settle, A Rai, <strong>R Antonova</strong>, J Bohg.&nbsp;<em>IEEE International Conference on Robotics and Automation (ICRA), 2022</em></p><div><a class="btn btn-primary btn-sm p-1 me-1 mt-1" role="button" href="https://ieeexplore.ieee.org/abstract/document/9812402">&nbsp;<i class="icon ion-ios-paper me-1 mt-1"></i>&nbsp;ICRA 2022 Paper&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://arxiv.org/abs/2109.14078" style="color: var(--bs-secondary);background: var(--bs-light-bg-subtle);">&nbsp;<i class="icon ion-ios-paper-outline me-1 mt-1"></i>&nbsp;arXiv&nbsp;</a><a class="btn btn-secondary btn-sm p-1 me-1 mt-1" role="button" href="https://yjy0625.github.io/projects/viptl/" style="background: var(--bs-light-bg-subtle);color: var(--bs-secondary);">&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" fill="currentColor" viewBox="0 0 16 16" class="bi bi-link-45deg">
  <path d="M4.715 6.542 3.343 7.914a3 3 0 1 0 4.243 4.243l1.828-1.829A3 3 0 0 0 8.586 5.5L8 6.086a1.002 1.002 0 0 0-.154.199 2 2 0 0 1 .861 3.337L6.88 11.45a2 2 0 1 1-2.83-2.83l.793-.792a4.018 4.018 0 0 1-.128-1.287z"></path>
  <path d="M6.586 4.672A3 3 0 0 0 7.414 9.5l.775-.776a2 2 0 0 1-.896-3.346L9.12 3.55a2 2 0 1 1 2.83 2.83l-.793.792c.112.42.155.855.128 1.287l1.372-1.372a3 3 0 1 0-4.243-4.243z"></path>
</svg>Website&nbsp;</a></div></div></div></div></section><footer class="pb-4 bg-primary-gradient" style="background: #ffffff;" data-bs-theme="light"><footer class="text-center bg-body"><div class="col"><ul class="list-inline my-2"></ul></div><div class="container"><div class="row row-cols-1 row-cols-lg-3"><div class="col"><p class="text-body my-2"></p></div><div class="col"><img class="object-fit-contain" src="assets/img/index/cambridge_logo.png" alt="Slide Image" width="211" height="59"></div><div class="col"></div></div></div></footer></footer><script src="assets/bootstrap/js/bootstrap.min.js"></script><script src="assets/js/script.min.js"></script></body></html>